{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f71e2557",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imports successful!\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Visualization script for the Rewrite Puzzle neural network predictor.\n",
        "\n",
        "This script answers:\n",
        "1. Where is the predictor (model) stored\n",
        "2. How does the predictor predict the outcome\n",
        "3. What is the input of the predictor, what is the output\n",
        "4. How to visualize the predictor's functionality with real outputs in vector form\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Add parent directory to path (works in Jupyter notebooks)\n",
        "current_dir = os.getcwd()\n",
        "if os.path.basename(current_dir) == 'rewrite_puzzle':\n",
        "    parent_dir = os.path.dirname(current_dir)\n",
        "    sys.path.insert(0, parent_dir)\n",
        "else:\n",
        "    # If already in parent directory, add current directory\n",
        "    sys.path.insert(0, current_dir)\n",
        "\n",
        "# Import using module path to handle relative imports in RewritePuzzleGame.py\n",
        "from rewrite_puzzle.RewritePuzzleGame import RewritePuzzleGame\n",
        "from rewrite_puzzle.pytorch.NNet import NNetWrapper\n",
        "from rewrite_puzzle.ReG_MCTS import MCTS\n",
        "from utils import dotdict\n",
        "\n",
        "print(\"Imports successful!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "37063bb1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Helper functions defined!\n"
          ]
        }
      ],
      "source": [
        "# Helper functions for visualization\n",
        "\n",
        "def print_section(title):\n",
        "    \"\"\"Print a formatted section header.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"  {title}\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "def visualize_model_storage():\n",
        "    \"\"\"Answer: Where is the predictor (model) stored?\"\"\"\n",
        "    print_section(\"1. WHERE IS THE PREDICTOR (MODEL) STORED?\")\n",
        "    \n",
        "    print(\"The model is stored as PyTorch checkpoint files (.pth.tar format).\")\n",
        "    print(\"\\nStorage locations:\")\n",
        "    print(\"  - Default checkpoint folder: './temp/rewrite_puzzle/'\")\n",
        "    print(\"  - Best model: './temp/rewrite_puzzle/best.pth.tar'\")\n",
        "    print(\"  - Iteration checkpoints: './temp/rewrite_puzzle/checkpoint_N.pth.tar'\")\n",
        "    print(\"  - Temporary model: './temp/rewrite_puzzle/temp.pth.tar'\")\n",
        "    \n",
        "    print(\"\\nModel structure:\")\n",
        "    print(\"  - The model contains the neural network's state_dict (weights and biases)\")\n",
        "    print(\"  - Saved using: torch.save({'state_dict': self.nnet.state_dict()}, filepath)\")\n",
        "    print(\"  - Loaded using: checkpoint = torch.load(filepath); self.nnet.load_state_dict(checkpoint['state_dict'])\")\n",
        "    \n",
        "    # Check if model exists\n",
        "    checkpoint_path = './temp/rewrite_puzzle/best.pth.tar'\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(f\"\\n✓ Found model at: {checkpoint_path}\")\n",
        "        file_size = os.path.getsize(checkpoint_path) / (1024 * 1024)  # MB\n",
        "        print(f\"  File size: {file_size:.2f} MB\")\n",
        "    else:\n",
        "        print(f\"\\n❌ No model found at: {checkpoint_path}\")\n",
        "        print(\"  (This is normal if training hasn't been run yet)\")\n",
        "\n",
        "\n",
        "def visualize_prediction_process(game, nnet):\n",
        "    \"\"\"Answer: How does the predictor predict the outcome?\"\"\"\n",
        "    print_section(\"2. HOW DOES THE PREDICTOR PREDICT THE OUTCOME?\")\n",
        "    \n",
        "    print(\"The prediction process has two main components:\")\n",
        "    print(\"\\nA. Neural Network Prediction:\")\n",
        "    print(\"  1. Input: Board state (1D numpy array)\")\n",
        "    print(\"  2. Forward pass through fully connected layers:\")\n",
        "    print(\"     - Input → FC1 (256 channels) → BatchNorm → ReLU → Dropout\")\n",
        "    print(\"     - → FC2 (256) → BatchNorm → ReLU → Dropout\")\n",
        "    print(\"     - → FC3 (256) → BatchNorm → ReLU → Dropout\")\n",
        "    print(\"     - → FC4 (256) → BatchNorm → ReLU → Dropout\")\n",
        "    print(\"     - → FC5 (512) → BatchNorm → ReLU → Dropout\")\n",
        "    print(\"     - → FC6 (256) → BatchNorm → ReLU → Dropout\")\n",
        "    print(\"  3. Two output heads:\")\n",
        "    print(\"     - Policy head (FC_pi): action_size logits → log_softmax → exp\")\n",
        "    print(\"     - Value head (FC_v): 1 value → tanh\")\n",
        "    \n",
        "    print(\"\\nB. MCTS Enhancement:\")\n",
        "    print(\"  1. Neural network provides initial policy (pi) and value (v)\")\n",
        "    print(\"  2. MCTS performs numMCTSSims simulations (default: 25)\")\n",
        "    print(\"  3. Each simulation:\")\n",
        "    print(\"     - Selects action with highest UCB (Upper Confidence Bound)\")\n",
        "    print(\"     - Expands leaf nodes using neural network predictions\")\n",
        "    print(\"     - Backpropagates values up the tree\")\n",
        "    print(\"  4. Final policy: normalized visit counts from MCTS tree\")\n",
        "    \n",
        "    print(\"\\nCode flow:\")\n",
        "    print(\"  nnet.predict(board) → (pi_raw, v_raw)\")\n",
        "    print(\"  mcts.getActionProb(board) → uses nnet.predict internally → returns improved policy\")\n",
        "\n",
        "\n",
        "def visualize_input_output(game, nnet):\n",
        "    \"\"\"Answer: What is the input and output of the predictor?\"\"\"\n",
        "    print_section(\"3. WHAT IS THE INPUT AND OUTPUT OF THE PREDICTOR?\")\n",
        "    \n",
        "    # Create a sample game state\n",
        "    game_instance = game(start_expr=\"1 + (2 * 3)\", goal_expr=\"(3 * 2) + 1\", max_steps=20)\n",
        "    board = game_instance.getInitBoard()\n",
        "    canonical_board = game_instance.getCanonicalForm(board, 1)\n",
        "    \n",
        "    print(\"INPUT:\")\n",
        "    print(f\"  Type: numpy.ndarray\")\n",
        "    print(f\"  Shape: {canonical_board.shape}\")\n",
        "    print(f\"  Dtype: {canonical_board.dtype}\")\n",
        "    print(f\"  Size: {canonical_board.size} elements\")\n",
        "    \n",
        "    print(\"\\n  Input encoding format:\")\n",
        "    print(\"    - Slots 0 to ~70% of max_expr_length: Current expression (ASCII codes / 128.0)\")\n",
        "    print(\"    - Slots ~70% to ~100%: Goal expression (ASCII codes / 128.0)\")\n",
        "    print(\"    - Slot -2: Steps taken (normalized to [0, 1])\")\n",
        "    print(\"    - Slot -1: Goal expression length (normalized)\")\n",
        "    \n",
        "    print(f\"\\n  Sample input (first 20 values):\")\n",
        "    print(f\"    {canonical_board[:20]}\")\n",
        "    print(f\"  Sample input (last 5 values):\")\n",
        "    print(f\"    {canonical_board[-5:]}\")\n",
        "    \n",
        "    # Get prediction\n",
        "    pi_raw, v_raw = nnet.predict(canonical_board)\n",
        "    \n",
        "    print(\"\\nOUTPUT:\")\n",
        "    print(\"  The predictor returns TWO outputs:\")\n",
        "    print(\"\\n  A. Policy Vector (pi):\")\n",
        "    print(f\"    Type: numpy.ndarray\")\n",
        "    print(f\"    Shape: {pi_raw.shape}\")\n",
        "    print(f\"    Dtype: {pi_raw.dtype}\")\n",
        "    print(f\"    Size: {pi_raw.size} elements (one per possible action)\")\n",
        "    print(f\"    Range: [0, 1] (probabilities, sum to ~1)\")\n",
        "    print(f\"    Meaning: Probability distribution over all possible actions\")\n",
        "    print(f\"    Processing: log_softmax → exp (ensures probabilities sum to 1)\")\n",
        "    \n",
        "    print(f\"\\n    Sample policy (top 10 actions):\")\n",
        "    top_indices = np.argsort(pi_raw)[::-1][:10]\n",
        "    for i, idx in enumerate(top_indices):\n",
        "        print(f\"      Action {idx:4d}: probability = {pi_raw[idx]:.6f}\")\n",
        "    \n",
        "    print(\"\\n  B. Value Scalar (v):\")\n",
        "    v_scalar = float(np.array(v_raw).item() if isinstance(v_raw, np.ndarray) else v_raw)\n",
        "    print(f\"    Type: numpy.ndarray (scalar)\")\n",
        "    print(f\"    Shape: {np.array(v_raw).shape}\")\n",
        "    print(f\"    Value: {v_scalar:.6f}\")\n",
        "    print(f\"    Range: [-1, 1] (tanh activation)\")\n",
        "    print(f\"    Meaning: Expected outcome from current state\")\n",
        "    print(f\"             - For single-player: probability of solving the puzzle\")\n",
        "    print(f\"             - +1 = likely to solve, -1 = likely to fail\")\n",
        "    print(f\"    Processing: Linear layer → tanh (bounded to [-1, 1])\")\n",
        "    \n",
        "    return canonical_board, pi_raw, v_raw\n",
        "\n",
        "\n",
        "def visualize_full_prediction(game, nnet, board_state):\n",
        "    \"\"\"Answer: How to visualize the predictor's functionality with real outputs?\"\"\"\n",
        "    print_section(\"4. VISUALIZING PREDICTOR FUNCTIONALITY - REAL OUTPUTS\")\n",
        "    \n",
        "    # Decode the board to show what it represents\n",
        "    board_obj = game._decode_state(board_state)\n",
        "    print(\"CURRENT GAME STATE:\")\n",
        "    print(f\"  Current expression: {board_obj.current_expr}\")\n",
        "    print(f\"  Goal expression: {board_obj.goal_expr}\")\n",
        "    print(f\"  Steps taken: {board_obj.steps_taken}/{board_obj.max_steps}\")\n",
        "    print(f\"  Is solved: {board_obj.is_solved()}\")\n",
        "    \n",
        "    # Get raw neural network prediction\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"STEP 1: RAW NEURAL NETWORK PREDICTION\")\n",
        "    print(\"-\"*80)\n",
        "    pi_raw, v_raw = nnet.predict(board_state)\n",
        "    v_scalar = float(np.array(v_raw).item() if isinstance(v_raw, np.ndarray) else v_raw)\n",
        "    \n",
        "    print(f\"\\nRaw Policy Vector (pi):\")\n",
        "    print(f\"  Shape: {pi_raw.shape}\")\n",
        "    print(f\"  Sum: {np.sum(pi_raw):.6f} (should be ~1.0)\")\n",
        "    print(f\"  Min: {np.min(pi_raw):.6f}, Max: {np.max(pi_raw):.6f}\")\n",
        "    print(f\"  Non-zero entries: {np.count_nonzero(pi_raw)}\")\n",
        "    \n",
        "    print(f\"\\nRaw Value (v):\")\n",
        "    print(f\"  Value: {v_scalar:.6f}\")\n",
        "    print(f\"  Interpretation: {'Likely to solve' if v_scalar > 0 else 'Likely to fail'}\")\n",
        "    \n",
        "    # Show valid moves\n",
        "    valids = game.getValidMoves(board_state, 1)\n",
        "    valid_actions = board_obj.get_all_valid_actions()\n",
        "    print(f\"\\nValid Actions: {len(valid_actions)} out of {game.getActionSize()} total actions\")\n",
        "    \n",
        "    # Mask policy with valid moves\n",
        "    pi_masked = pi_raw * valids\n",
        "    if np.sum(pi_masked) > 0:\n",
        "        pi_masked = pi_masked / np.sum(pi_masked)\n",
        "    \n",
        "    print(f\"\\nMasked Policy (only valid moves):\")\n",
        "    print(f\"  Sum: {np.sum(pi_masked):.6f}\")\n",
        "    print(f\"  Top 5 valid actions:\")\n",
        "    max_positions = game.max_expr_length // 2\n",
        "    action_scores = []\n",
        "    for rule_idx, path in valid_actions:\n",
        "        position_idx = len(path) % max_positions\n",
        "        action = rule_idx * max_positions + position_idx\n",
        "        if action < len(pi_masked):\n",
        "            prob = pi_masked[action]\n",
        "            try:\n",
        "                if hasattr(board_obj, 'rules') and rule_idx < len(board_obj.rules):\n",
        "                    rule_name = board_obj.rules[rule_idx].name\n",
        "                else:\n",
        "                    rule_name = f\"Rule{rule_idx}\"\n",
        "            except (AttributeError, IndexError):\n",
        "                rule_name = f\"Rule{rule_idx}\"\n",
        "            action_scores.append((prob, rule_idx, path, rule_name, action))\n",
        "    \n",
        "    action_scores.sort(reverse=True, key=lambda x: x[0])\n",
        "    for i, (prob, rule_idx, path, rule_name, action) in enumerate(action_scores[:5]):\n",
        "        print(f\"    {i+1}. Action {action:4d} ({rule_name:20s}): {prob:.6f}\")\n",
        "    \n",
        "    # MCTS enhanced prediction\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"STEP 2: MCTS-ENHANCED PREDICTION\")\n",
        "    print(\"-\"*80)\n",
        "    args = dotdict({\n",
        "        'numMCTSSims': 25,\n",
        "        'cpuct': 1,\n",
        "    })\n",
        "    mcts = MCTS(game, nnet, args)\n",
        "    pi_mcts = mcts.getActionProb(board_state, temp=0)\n",
        "    # Convert to numpy array if it's a list\n",
        "    if isinstance(pi_mcts, list):\n",
        "        pi_mcts = np.array(pi_mcts)\n",
        "    \n",
        "    print(f\"\\nMCTS Policy Vector (pi_mcts):\")\n",
        "    print(f\"  Shape: {pi_mcts.shape}\")\n",
        "    print(f\"  Sum: {np.sum(pi_mcts):.6f} (should be 1.0)\")\n",
        "    print(f\"  Min: {np.min(pi_mcts):.6f}, Max: {np.max(pi_mcts):.6f}\")\n",
        "    print(f\"  Non-zero entries: {np.count_nonzero(pi_mcts)}\")\n",
        "    \n",
        "    print(f\"\\nTop 5 actions after MCTS:\")\n",
        "    mcts_action_scores = []\n",
        "    for rule_idx, path in valid_actions:\n",
        "        position_idx = len(path) % max_positions\n",
        "        action = rule_idx * max_positions + position_idx\n",
        "        if action < len(pi_mcts):\n",
        "            prob = pi_mcts[action]\n",
        "            try:\n",
        "                if hasattr(board_obj, 'rules') and rule_idx < len(board_obj.rules):\n",
        "                    rule_name = board_obj.rules[rule_idx].name\n",
        "                else:\n",
        "                    rule_name = f\"Rule{rule_idx}\"\n",
        "            except (AttributeError, IndexError):\n",
        "                rule_name = f\"Rule{rule_idx}\"\n",
        "            mcts_action_scores.append((prob, rule_idx, path, rule_name, action))\n",
        "    \n",
        "    mcts_action_scores.sort(reverse=True, key=lambda x: x[0])\n",
        "    for i, (prob, rule_idx, path, rule_name, action) in enumerate(mcts_action_scores[:5]):\n",
        "        print(f\"    {i+1}. Action {action:4d} ({rule_name:20s}): {prob:.6f}\")\n",
        "    \n",
        "    # Comparison\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"COMPARISON: Raw NN vs MCTS-Enhanced\")\n",
        "    print(\"-\"*80)\n",
        "    best_raw = np.argmax(pi_masked)\n",
        "    best_mcts = np.argmax(pi_mcts)\n",
        "    print(f\"\\nBest action (Raw NN):    {best_raw} (prob: {pi_masked[best_raw]:.6f})\")\n",
        "    print(f\"Best action (MCTS):       {best_mcts} (prob: {pi_mcts[best_mcts]:.6f})\")\n",
        "    print(f\"Same action?             {'Yes' if best_raw == best_mcts else 'No'}\")\n",
        "    \n",
        "    # Full vector output\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"FULL VECTOR OUTPUTS\")\n",
        "    print(\"-\"*80)\n",
        "    print(\"\\nRaw Policy Vector (first 50 values):\")\n",
        "    print(pi_raw[:50])\n",
        "    print(\"\\nRaw Policy Vector (last 50 values):\")\n",
        "    print(pi_raw[-50:])\n",
        "    \n",
        "    print(\"\\nMCTS Policy Vector (first 50 values):\")\n",
        "    print(pi_mcts[:50])\n",
        "    print(\"\\nMCTS Policy Vector (last 50 values):\")\n",
        "    print(pi_mcts[-50:])\n",
        "    \n",
        "    return pi_raw, v_raw, pi_mcts\n",
        "\n",
        "print(\"Helper functions defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "480de0c9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "  REWRITE PUZZLE NEURAL NETWORK PREDICTOR VISUALIZATION\n",
            "================================================================================\n",
            "\n",
            "Initializing game and neural network...\n",
            "No trained model found. Using untrained model (random weights).\n",
            "  (To train a model, run ReG_main_rewrite_puzzle.py)\n",
            "\n",
            "================================================================================\n",
            "  1. WHERE IS THE PREDICTOR (MODEL) STORED?\n",
            "================================================================================\n",
            "\n",
            "The model is stored as PyTorch checkpoint files (.pth.tar format).\n",
            "\n",
            "Storage locations:\n",
            "  - Default checkpoint folder: './temp/rewrite_puzzle/'\n",
            "  - Best model: './temp/rewrite_puzzle/best.pth.tar'\n",
            "  - Iteration checkpoints: './temp/rewrite_puzzle/checkpoint_N.pth.tar'\n",
            "  - Temporary model: './temp/rewrite_puzzle/temp.pth.tar'\n",
            "\n",
            "Model structure:\n",
            "  - The model contains the neural network's state_dict (weights and biases)\n",
            "  - Saved using: torch.save({'state_dict': self.nnet.state_dict()}, filepath)\n",
            "  - Loaded using: checkpoint = torch.load(filepath); self.nnet.load_state_dict(checkpoint['state_dict'])\n",
            "\n",
            "❌ No model found at: ./temp/rewrite_puzzle/best.pth.tar\n",
            "  (This is normal if training hasn't been run yet)\n",
            "\n",
            "================================================================================\n",
            "  2. HOW DOES THE PREDICTOR PREDICT THE OUTCOME?\n",
            "================================================================================\n",
            "\n",
            "The prediction process has two main components:\n",
            "\n",
            "A. Neural Network Prediction:\n",
            "  1. Input: Board state (1D numpy array)\n",
            "  2. Forward pass through fully connected layers:\n",
            "     - Input → FC1 (256 channels) → BatchNorm → ReLU → Dropout\n",
            "     - → FC2 (256) → BatchNorm → ReLU → Dropout\n",
            "     - → FC3 (256) → BatchNorm → ReLU → Dropout\n",
            "     - → FC4 (256) → BatchNorm → ReLU → Dropout\n",
            "     - → FC5 (512) → BatchNorm → ReLU → Dropout\n",
            "     - → FC6 (256) → BatchNorm → ReLU → Dropout\n",
            "  3. Two output heads:\n",
            "     - Policy head (FC_pi): action_size logits → log_softmax → exp\n",
            "     - Value head (FC_v): 1 value → tanh\n",
            "\n",
            "B. MCTS Enhancement:\n",
            "  1. Neural network provides initial policy (pi) and value (v)\n",
            "  2. MCTS performs numMCTSSims simulations (default: 25)\n",
            "  3. Each simulation:\n",
            "     - Selects action with highest UCB (Upper Confidence Bound)\n",
            "     - Expands leaf nodes using neural network predictions\n",
            "     - Backpropagates values up the tree\n",
            "  4. Final policy: normalized visit counts from MCTS tree\n",
            "\n",
            "Code flow:\n",
            "  nnet.predict(board) → (pi_raw, v_raw)\n",
            "  mcts.getActionProb(board) → uses nnet.predict internally → returns improved policy\n",
            "\n",
            "================================================================================\n",
            "  3. WHAT IS THE INPUT AND OUTPUT OF THE PREDICTOR?\n",
            "================================================================================\n",
            "\n",
            "INPUT:\n",
            "  Type: numpy.ndarray\n",
            "  Shape: (202,)\n",
            "  Dtype: float32\n",
            "  Size: 202 elements\n",
            "\n",
            "  Input encoding format:\n",
            "    - Slots 0 to ~70% of max_expr_length: Current expression (ASCII codes / 128.0)\n",
            "    - Slots ~70% to ~100%: Goal expression (ASCII codes / 128.0)\n",
            "    - Slot -2: Steps taken (normalized to [0, 1])\n",
            "    - Slot -1: Goal expression length (normalized)\n",
            "\n",
            "  Sample input (first 20 values):\n",
            "    [0.3125    0.3828125 0.25      0.3359375 0.25      0.3125    0.390625\n",
            " 0.25      0.328125  0.25      0.3984375 0.3203125 0.3203125 0.\n",
            " 0.        0.        0.        0.        0.        0.       ]\n",
            "  Sample input (last 5 values):\n",
            "    [0.    0.    0.    0.    0.013]\n",
            "\n",
            "OUTPUT:\n",
            "  The predictor returns TWO outputs:\n",
            "\n",
            "  A. Policy Vector (pi):\n",
            "    Type: numpy.ndarray\n",
            "    Shape: (700,)\n",
            "    Dtype: float32\n",
            "    Size: 700 elements (one per possible action)\n",
            "    Range: [0, 1] (probabilities, sum to ~1)\n",
            "    Meaning: Probability distribution over all possible actions\n",
            "    Processing: log_softmax → exp (ensures probabilities sum to 1)\n",
            "\n",
            "    Sample policy (top 10 actions):\n",
            "      Action  597: probability = 0.001554\n",
            "      Action  397: probability = 0.001551\n",
            "      Action  626: probability = 0.001546\n",
            "      Action  261: probability = 0.001545\n",
            "      Action  605: probability = 0.001544\n",
            "      Action  363: probability = 0.001540\n",
            "      Action  163: probability = 0.001538\n",
            "      Action  127: probability = 0.001537\n",
            "      Action  279: probability = 0.001534\n",
            "      Action  508: probability = 0.001534\n",
            "\n",
            "  B. Value Scalar (v):\n",
            "    Type: numpy.ndarray (scalar)\n",
            "    Shape: (1,)\n",
            "    Value: -0.001019\n",
            "    Range: [-1, 1] (tanh activation)\n",
            "    Meaning: Expected outcome from current state\n",
            "             - For single-player: probability of solving the puzzle\n",
            "             - +1 = likely to solve, -1 = likely to fail\n",
            "    Processing: Linear layer → tanh (bounded to [-1, 1])\n",
            "\n",
            "================================================================================\n",
            "  4. VISUALIZING PREDICTOR FUNCTIONALITY - REAL OUTPUTS\n",
            "================================================================================\n",
            "\n",
            "CURRENT GAME STATE:\n",
            "  Current expression: (1 + (2 * 3))\n",
            "  Goal expression: ((3 * 2) + 1)\n",
            "  Steps taken: 0/20\n",
            "  Is solved: False\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "STEP 1: RAW NEURAL NETWORK PREDICTION\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Raw Policy Vector (pi):\n",
            "  Shape: (700,)\n",
            "  Sum: 1.000000 (should be ~1.0)\n",
            "  Min: 0.001320, Max: 0.001554\n",
            "  Non-zero entries: 700\n",
            "\n",
            "Raw Value (v):\n",
            "  Value: -0.001019\n",
            "  Interpretation: Likely to fail\n",
            "\n",
            "Valid Actions: 2 out of 700 total actions\n",
            "\n",
            "Masked Policy (only valid moves):\n",
            "  Sum: 1.000000\n",
            "  Top 5 valid actions:\n",
            "    1. Action  601 (eval_mult_leaves    ): 0.508290\n",
            "    2. Action  200 (commute_add         ): 0.491710\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "STEP 2: MCTS-ENHANCED PREDICTION\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "MCTS Policy Vector (pi_mcts):\n",
            "  Shape: (700,)\n",
            "  Sum: 1.000000 (should be 1.0)\n",
            "  Min: 0.000000, Max: 1.000000\n",
            "  Non-zero entries: 1\n",
            "\n",
            "Top 5 actions after MCTS:\n",
            "    1. Action  601 (eval_mult_leaves    ): 1.000000\n",
            "    2. Action  200 (commute_add         ): 0.000000\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "COMPARISON: Raw NN vs MCTS-Enhanced\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Best action (Raw NN):    601 (prob: 0.508290)\n",
            "Best action (MCTS):       601 (prob: 1.000000)\n",
            "Same action?             Yes\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "FULL VECTOR OUTPUTS\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Raw Policy Vector (first 50 values):\n",
            "[0.00138218 0.0014982  0.00143125 0.00140422 0.00146042 0.00133657\n",
            " 0.0013242  0.00134996 0.00145269 0.00147547 0.0014324  0.00138739\n",
            " 0.00145576 0.00144786 0.00147675 0.00141999 0.00144425 0.00135126\n",
            " 0.0014941  0.00144541 0.00141474 0.00149643 0.00150295 0.0013492\n",
            " 0.00139433 0.00146124 0.00139043 0.00136572 0.00151532 0.00140452\n",
            " 0.00147245 0.0014126  0.00147159 0.00152955 0.00142447 0.00151163\n",
            " 0.00145056 0.00138568 0.00136672 0.00132388 0.00152703 0.00145607\n",
            " 0.00135126 0.00144017 0.0013879  0.00137672 0.00147303 0.00145103\n",
            " 0.00146169 0.00143302]\n",
            "\n",
            "Raw Policy Vector (last 50 values):\n",
            "[0.00146154 0.00151657 0.00134407 0.00138528 0.00145859 0.00142688\n",
            " 0.00138586 0.00138863 0.00143259 0.00142834 0.00137054 0.00144901\n",
            " 0.00139707 0.00140699 0.00146716 0.00140609 0.00138267 0.00135831\n",
            " 0.00143041 0.00140151 0.00140132 0.00136423 0.00146612 0.00147202\n",
            " 0.00145091 0.00148325 0.00143929 0.00140247 0.00142896 0.00134478\n",
            " 0.00142382 0.00140111 0.00138658 0.00148599 0.00136733 0.00143099\n",
            " 0.00148042 0.001426   0.00145296 0.00135951 0.00140524 0.00146129\n",
            " 0.00150913 0.00140682 0.00138919 0.00134418 0.00145451 0.00141972\n",
            " 0.00132098 0.00135618]\n",
            "\n",
            "MCTS Policy Vector (first 50 values):\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "\n",
            "MCTS Policy Vector (last 50 values):\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "\n",
            "================================================================================\n",
            "  VISUALIZATION COMPLETE\n",
            "================================================================================\n",
            "\n",
            "Summary:\n",
            "  1. Models are stored in './temp/rewrite_puzzle/' as .pth.tar files\n",
            "  2. Prediction uses a fully connected neural network + MCTS\n",
            "  3. Input: 1D array encoding game state, Output: policy vector + value scalar\n",
            "  4. Use nnet.predict(board) to get raw predictions, mcts.getActionProb() for MCTS-enhanced\n"
          ]
        }
      ],
      "source": [
        "# Main execution: Run all visualizations\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"  REWRITE PUZZLE NEURAL NETWORK PREDICTOR VISUALIZATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Initialize game and network\n",
        "print(\"\\nInitializing game and neural network...\")\n",
        "game = RewritePuzzleGame\n",
        "game_instance = game(start_expr=\"1 + (2 * 3)\", goal_expr=\"(3 * 2) + 1\", max_steps=20)\n",
        "nnet = NNetWrapper(game_instance)\n",
        "\n",
        "# Check if model exists and try to load it\n",
        "checkpoint_path = './temp/rewrite_puzzle/best.pth.tar'\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(f\"Loading model from {checkpoint_path}...\")\n",
        "    try:\n",
        "        nnet.load_checkpoint('./temp/rewrite_puzzle/', 'best.pth.tar')\n",
        "        print(\"✓ Model loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading model: {e}\")\n",
        "        print(\"  Using untrained model (random weights)\")\n",
        "else:\n",
        "    print(\"No trained model found. Using untrained model (random weights).\")\n",
        "    print(\"  (To train a model, run ReG_main_rewrite_puzzle.py)\")\n",
        "\n",
        "# Run visualizations\n",
        "visualize_model_storage()\n",
        "visualize_prediction_process(game, nnet)\n",
        "board_state, pi_raw, v_raw = visualize_input_output(game, nnet)  # Pass class, not instance\n",
        "visualize_full_prediction(game_instance, nnet, board_state)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"  VISUALIZATION COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nSummary:\")\n",
        "print(\"  1. Models are stored in './temp/rewrite_puzzle/' as .pth.tar files\")\n",
        "print(\"  2. Prediction uses a fully connected neural network + MCTS\")\n",
        "print(\"  3. Input: 1D array encoding game state, Output: policy vector + value scalar\")\n",
        "print(\"  4. Use nnet.predict(board) to get raw predictions, mcts.getActionProb() for MCTS-enhanced\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
