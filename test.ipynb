{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2009f6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "exceptions must derive from BaseException",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      6\u001b[39m nnet = nn(g)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# # Load the best model\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# nnet.load_checkpoint('./temp/rewrite_puzzle/', 'best.pth.tar')\u001b[39;00m\n\u001b[32m     10\u001b[39m \n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Or load a specific iteration\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mnnet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./temp/rewrite_puzzle/\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcheckpoint_50.pth.tar\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/Files/Research New chapter/alpha-zero-general/rewrite_puzzle/pytorch/NNet.py:121\u001b[39m, in \u001b[36mNNetWrapper.load_checkpoint\u001b[39m\u001b[34m(self, folder, filename)\u001b[39m\n\u001b[32m    119\u001b[39m filepath = os.path.join(folder, filename)\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(filepath):\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mNo model in path \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(filepath))\n\u001b[32m    122\u001b[39m map_location = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m args.cuda \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    123\u001b[39m checkpoint = torch.load(filepath, map_location=map_location)\n",
      "\u001b[31mTypeError\u001b[39m: exceptions must derive from BaseException"
     ]
    }
   ],
   "source": [
    "from rewrite_puzzle.RewritePuzzleGame import RewritePuzzleGame as Game\n",
    "from rewrite_puzzle.pytorch.NNet import NNetWrapper as nn\n",
    "\n",
    "# Create game and network\n",
    "g = Game(start_expr=\"1 + 2 * 3\", goal_expr=7, max_steps=20)\n",
    "nnet = nn(g)\n",
    "\n",
    "# # Load the best model\n",
    "# nnet.load_checkpoint('./temp/rewrite_puzzle/', 'best.pth.tar')\n",
    "\n",
    "# Or load a specific iteration\n",
    "nnet.load_checkpoint('./temp/rewrite_puzzle/', 'checkpoint_50.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b32c66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "\n",
      "============================================================\n",
      "Starting game visualization\n",
      "============================================================\n",
      "\n",
      "--- Step 1 ---\n",
      "Current expression: (1 + 2)\n",
      "Goal: (2 + 1)\n",
      "Steps taken: 0/5\n",
      "Model's value prediction (win probability): 0.0006\n",
      "Raw policy (before MCTS) top action probability: 0.5062\n",
      "\n",
      "Valid moves (2):\n",
      "\n",
      "Top 2 moves (by model preference):\n",
      "  1. eval_add_leaves      (path: []) - Probability: 1.0000\n",
      "  2. commute_add          (path: []) - Probability: 0.0000\n",
      "\n",
      "Model's chosen action: 500 (probability: 1.0000)\n",
      "\n",
      "--- Step 2 ---\n",
      "Current expression: 3\n",
      "Goal: (2 + 1)\n",
      "Steps taken: 1/5\n",
      "Model's value prediction (win probability): 0.0005\n",
      "Raw policy (before MCTS) top action probability: 1.0000\n",
      "\n",
      "Valid moves (0):\n",
      "\n",
      "Top 0 moves (by model preference):\n",
      "\n",
      "Model's chosen action: 699 (probability: 1.0000)\n",
      "\n",
      "--- Step 3 ---\n",
      "Current expression: 3\n",
      "Goal: (2 + 1)\n",
      "Steps taken: 2/5\n",
      "Model's value prediction (win probability): 0.0005\n",
      "Raw policy (before MCTS) top action probability: 1.0000\n",
      "\n",
      "Valid moves (0):\n",
      "\n",
      "Top 0 moves (by model preference):\n",
      "\n",
      "Model's chosen action: 699 (probability: 1.0000)\n",
      "\n",
      "--- Step 4 ---\n",
      "Current expression: 3\n",
      "Goal: (2 + 1)\n",
      "Steps taken: 3/5\n",
      "Model's value prediction (win probability): 0.0005\n",
      "Raw policy (before MCTS) top action probability: 1.0000\n",
      "\n",
      "Valid moves (0):\n",
      "\n",
      "Top 0 moves (by model preference):\n",
      "\n",
      "Model's chosen action: 699 (probability: 1.0000)\n",
      "\n",
      "--- Step 5 ---\n",
      "Current expression: 3\n",
      "Goal: (2 + 1)\n",
      "Steps taken: 4/5\n",
      "Model's value prediction (win probability): 0.0005\n",
      "Raw policy (before MCTS) top action probability: 1.0000\n",
      "\n",
      "Valid moves (0):\n",
      "\n",
      "Top 0 moves (by model preference):\n",
      "\n",
      "Model's chosen action: 699 (probability: 1.0000)\n",
      "\n",
      "✗ Game lost (max steps reached)\n",
      "\n",
      "============================================================\n",
      "Game finished\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple script to visualize what the model is doing at each step.\n",
    "Shows the model's predictions, action probabilities, and decisions.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from rewrite_puzzle.RewritePuzzleGame import RewritePuzzleGame\n",
    "from rewrite_puzzle.pytorch.NNet import NNetWrapper\n",
    "from MCTS import MCTS\n",
    "from utils import dotdict\n",
    "\n",
    "# Load the model\n",
    "print(\"Loading model...\")\n",
    "game = RewritePuzzleGame(start_expr=\"(1 + 2)\", goal_expr=\"(2 + 1)\", max_steps=5)\n",
    "nnet = NNetWrapper(game)\n",
    "nnet.load_checkpoint('./temp/rewrite_puzzle/', 'temp.pth.tar')\n",
    "\n",
    "# Setup MCTS\n",
    "args = dotdict({\n",
    "    'numMCTSSims': 25,\n",
    "    'cpuct': 1,\n",
    "})\n",
    "mcts = MCTS(game, nnet, args)\n",
    "\n",
    "# Start a game\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Starting game visualization\")\n",
    "print(\"=\"*60)\n",
    "board = game.getInitBoard()\n",
    "step = 0\n",
    "max_steps = 20\n",
    "\n",
    "while step < max_steps:\n",
    "    step += 1\n",
    "    print(f\"\\n--- Step {step} ---\")\n",
    "    \n",
    "    # Decode current state\n",
    "    board_obj = game._decode_state(board)\n",
    "    current_expr = str(board_obj.current_expr)\n",
    "    goal_expr = str(board_obj.goal_expr)\n",
    "    current_value = board_obj.current_expr.evaluate()\n",
    "    \n",
    "    print(f\"Current expression: {current_expr}\")\n",
    "    print(f\"Goal: {goal_expr}\")\n",
    "    print(f\"Steps taken: {board_obj.steps_taken}/{board_obj.max_steps}\")\n",
    "    \n",
    "    # Get raw neural network prediction (show even if solved)\n",
    "    canonical_board = game.getCanonicalForm(board, 1)\n",
    "    pi_raw, v = nnet.predict(canonical_board)\n",
    "    \n",
    "    # Extract scalar value (handle numpy array/scalar)\n",
    "    v_scalar = float(np.array(v).item() if isinstance(v, np.ndarray) else v)\n",
    "    print(f\"Model's value prediction (win probability): {v_scalar:.4f}\")\n",
    "    \n",
    "    # Check if solved\n",
    "    if board_obj.is_solved():\n",
    "        print(\"✓ SOLVED!\")\n",
    "        # Still show what moves would be available (even though game is solved)\n",
    "        valid_actions = board_obj.get_all_valid_actions()\n",
    "        if len(valid_actions) > 0:\n",
    "            print(f\"\\nNote: {len(valid_actions)} valid moves were available, but game is already solved.\")\n",
    "        break\n",
    "    \n",
    "    # Show raw policy (before MCTS) for top actions\n",
    "    valids_raw = game.getValidMoves(board, 1)\n",
    "    pi_raw_masked = pi_raw * valids_raw\n",
    "    if np.sum(pi_raw_masked) > 0:\n",
    "        pi_raw_masked = pi_raw_masked / np.sum(pi_raw_masked)  # Normalize\n",
    "    top_raw_idx = np.argmax(pi_raw_masked)\n",
    "    print(f\"Raw policy (before MCTS) top action probability: {pi_raw_masked[top_raw_idx]:.4f}\")\n",
    "    \n",
    "    # Get valid moves\n",
    "    valids = game.getValidMoves(board, 1)\n",
    "    valid_actions = board_obj.get_all_valid_actions()\n",
    "    \n",
    "    print(f\"\\nValid moves ({len(valid_actions)}):\")\n",
    "    max_positions = game.max_expr_length // 2\n",
    "    \n",
    "    # Reset MCTS tree for clean predictions\n",
    "    mcts = MCTS(game, nnet, args)\n",
    "    \n",
    "    # Get MCTS action probabilities\n",
    "    action_probs = mcts.getActionProb(canonical_board, temp=0)\n",
    "    \n",
    "    # Show top moves\n",
    "    top_n = min(5, len(valid_actions))\n",
    "    action_scores = []\n",
    "    \n",
    "    for rule_idx, path in valid_actions:\n",
    "        position_idx = len(path) % max_positions\n",
    "        action = rule_idx * max_positions + position_idx\n",
    "        if action < len(action_probs):\n",
    "            prob = action_probs[action]\n",
    "            # Get rule name safely\n",
    "            try:\n",
    "                if hasattr(board_obj, 'rules') and rule_idx < len(board_obj.rules):\n",
    "                    rule_name = board_obj.rules[rule_idx].name\n",
    "                else:\n",
    "                    rule_name = f\"Rule{rule_idx}\"\n",
    "            except (AttributeError, IndexError):\n",
    "                rule_name = f\"Rule{rule_idx}\"\n",
    "            action_scores.append((prob, rule_idx, path, rule_name, action))\n",
    "    \n",
    "    # Sort by probability\n",
    "    action_scores.sort(reverse=True, key=lambda x: x[0])\n",
    "    \n",
    "    print(f\"\\nTop {top_n} moves (by model preference):\")\n",
    "    for i, (prob, rule_idx, path, rule_name, action) in enumerate(action_scores[:top_n]):\n",
    "        print(f\"  {i+1}. {rule_name:20s} (path: {path}) - Probability: {prob:.4f}\")\n",
    "    \n",
    "    # Choose best action\n",
    "    best_action_idx = np.argmax(action_probs)\n",
    "    print(f\"\\nModel's chosen action: {best_action_idx} (probability: {action_probs[best_action_idx]:.4f})\")\n",
    "    \n",
    "    # Apply the action\n",
    "    board, _ = game.getNextState(board, 1, best_action_idx)\n",
    "    \n",
    "    # Check if game ended\n",
    "    result = game.getGameEnded(board, 1)\n",
    "    if result != 0:\n",
    "        if result == 1:\n",
    "            print(\"\\n✓ Game won!\")\n",
    "        else:\n",
    "            print(\"\\n✗ Game lost (max steps reached)\")\n",
    "        break\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Game finished\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
